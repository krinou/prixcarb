name: Générer stations.json manuellement

on:
  workflow_dispatch:

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      # 1) Récupérer le dépôt
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # 2) Installer Python
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          

      # 2.1) Installer les dépendances
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4

      # 3) Télécharger le flux instantané (ZIP)
      # Réf. ancienne page open data : https://www.prix-carburants.gouv.fr/rubrique/opendata/
      - name: Download carburant data (XML in ZIP)
        run: |
          set -e
          curl -L -o data.zip "https://donnees.roulez-eco.fr/opendata/instantane"

      # 4) Extraire le XML
      - name: Extract XML
        run: |
          set -e
          unzip -o data.zip
          ls -l

      # 5) Script de conversion (avec enseignes)
      - name: Create convert.py
        shell: bash
        run: |
          set -euo pipefail
          cat > convert.py << 'EOF'
          #!/usr/bin/env python3
          # -*- coding: utf-8 -*-

          import os
          import sys
          import json
          import time
          import xml.etree.ElementTree as ET
          from functools import lru_cache

          import requests
          from bs4 import BeautifulSoup

          # --------- Réglages ---------
          XML_CANDIDATES = [
              "PrixCarburants_instantane.xml",
              "prixcarburants_instantane.xml",
          ]
          OUTPUT_JSON = "stations.json"
          CACHE_FILE = "brand_cache.json"
          HEADERS = {
              "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36"
          }

          # --------- Cache des enseignes ---------
          def load_cache():
              if os.path.exists(CACHE_FILE):
                  try:
                      with open(CACHE_FILE, "r", encoding="utf-8") as f:
                          return json.load(f)
                  except Exception:
                      return {}
              return {}

          def save_cache(cache: dict):
              try:
                  with open(CACHE_FILE, "w", encoding="utf-8") as f:
                      json.dump(cache, f, ensure_ascii=False, indent=2)
              except Exception:
                  pass

          BRAND_CACHE = load_cache()

          def _save_brand(station_id: str, brand: str):
              BRAND_CACHE[station_id] = brand
              # sauvegarde au fil de l’eau
              save_cache(BRAND_CACHE)

          # --------- Scraping enseigne (à adapter si besoin) ---------
          def parse_brand_from_html(html: str) -> str:
              soup = BeautifulSoup(html, "html.parser")
              # heuristiques simples
              for sel in ['[itemprop="name"]', ".brand", ".station-brand", "h1", "h2", "title"]:
                  el = soup.select_one(sel) if sel != "title" else (soup.title if soup.title else None)
                  if el:
                      txt = (el.get_text(strip=True) if sel != "title" else el.string.strip() if el.string else "").strip()
                      if txt:
                          return txt
              return "Inconnue"

          @lru_cache(maxsize=10000)
          def get_brand_from_html(station_id: str, retries: int = 3, delay: float = 0.8) -> str:
              if not station_id:
                  return "Inconnue"
              # cache disque
              if station_id in BRAND_CACHE and BRAND_CACHE[station_id]:
                  return BRAND_CACHE[station_id]
              # URL à vérifier/adapter si besoin
              url_candidates = [
                  f"https://www.prix-carburants.gouv.fr/station/{station_id}",
                  f"https://www.prix-carburants.gouv.fr/{station_id}",
              ]
              for url in url_candidates:
                  for attempt in range(1, retries + 1):
                      try:
                          resp = requests.get(url, headers=HEADERS, timeout=15)
                          if resp.status_code != 200 or not resp.text:
                              time.sleep(delay * attempt)
                              continue
                          brand = parse_brand_from_html(resp.text).strip()
                          if brand and brand.lower() != "inconnue":
                              _save_brand(station_id, brand)
                              return brand
                      except Exception:
                          time.sleep(delay * attempt)
              _save_brand(station_id, "")
              return "Inconnue"

          # --------- Lecture XML OpenData ---------
          def find_xml_path() -> str:
              # Cherche un des noms standard
              for cand in XML_CANDIDATES:
                  if os.path.exists(cand):
                      return cand
              # Sinon, cherche le premier .xml à la racine
              for f in os.listdir("."):
                  if f.lower().endswith(".xml"):
                      return f
              raise FileNotFoundError("Fichier XML introuvable après extraction du ZIP.")

          def parse_xml(xml_path: str):
              """
              Retourne une liste de dicts 'stations-prix' aplatis :
              id, adresse, cp, ville, latitude, longitude, carburant, prix, date_maj
              """
              tree = ET.parse(xml_path)
              root = tree.getroot()
              rows = []
              for pdv in root.findall(".//pdv"):
                  sid = pdv.get("id")
                  lat = pdv.get("latitude")
                  lon = pdv.get("longitude")
                  cp = pdv.get("cp")
                  adresse_el = pdv.find("adresse")
                  ville_el = pdv.find("ville")
                  adresse = (adresse_el.text or "").strip() if adresse_el is not None else ""
                  ville = (ville_el.text or "").strip() if ville_el is not None else ""
                  for prix in pdv.findall("prix"):
                      carburant = prix.get("nom") or ""
                      valeur = prix.get("valeur") or ""
                      maj = prix.get("maj") or ""
                      rows.append({
                          "id": sid,
                          "adresse": adresse,
                          "cp": cp,
                          "ville": ville,
                          "latitude": lat,
                          "longitude": lon,
                          "carburant": carburant,
                          "prix": valeur,
                          "date_maj": maj,
                      })
              return rows

          # --------- Enrichissement + sortie ---------
          def main():
              xml_path = find_xml_path()
              rows = parse_xml(xml_path)

              # Dédupliquer les SIDs pour limiter les requêtes
              unique_sids = {r["id"] for r in rows if r.get("id")}
              for sid in unique_sids:
                  _ = get_brand_from_html(sid)

              # Ajoute la colonne 'enseigne'
              for r in rows:
                  sid = r.get("id")
                  r["enseigne"] = get_brand_from_html(sid) if sid else "Inconnue"

              with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
                  json.dump(rows, f, ensure_ascii=False, indent=2)

              # Sauvegarde cache finale
              save_cache(BRAND_CACHE)

              print(f"[OK] {len(rows)} lignes écrites dans {OUTPUT_JSON}")

          if __name__ == "__main__":
              main()
          EOF
          chmod +x convert.py

      # 6) Exécuter la conversion
      - name: Run convert.py
        run: |
          set -e
          python3 convert.py
          test -f stations.json && echo "stations.json présent"

      # (Optionnel) Aperçu rapide
      - name: Show first lines
        run: |
          if command -v jq >/dev/null 2>&1; then
            jq '.[0:5]' stations.json
          else
            head -n 80 stations.json || true
          fi

      # 7) Commit + push uniquement si changements
      - name: Commit and push stations.json (if changed)
        run: |
          set -e
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add stations.json brand_cache.json || true
          if git diff --cached --quiet; then
            echo "Aucun changement à pousser."
          else
            git commit -m "Mise à jour manuelle stations.json (+ cache enseignes)"
            git push
          fi

      # 8) Publier l'artifact (pratique pour vérifier)
      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: stations-json
          path: stations.json
